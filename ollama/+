from ollama import Client
from config_conversational import ConversationalConfigManager, Options, Prompt
import time
import argparse
import sys


class ConversationalLlmManager():
    def __init__(self, options: Options, active_prompt: Prompt):
        self.options = options
        self.active_prompt = active_prompt
        self.client = Client(
            host=self.options.host
        )
        self.eos: bool = False
        self.messages: list = []
        self.last_user_message = self.add_message("user", "")
        self.last_assistant_message = self.add_message("user", "")

    def update_active_prompt(self, active_prompt: Prompt):
        self.active_prompt = active_prompt
        self.clear_messages()

    def update_options(self, options: Options):
        self.options = options

    def add_message(self, role: str, content: str):
        message = {
            "role": role,
            "content": content
        }
        return message

    def clear_messages(self):
        self.messages.clear()
        self.messages += self.add_message("system", self.get_formatted_prompt(self.active_prompt))

    def get_formatted_prompt(self, prompt_name: str) -> str:
        formatted_prompt = self.active_prompt.prompt

        for append in self.active_prompt.appends:
            if append.language:
                formatted_prompt += f" The answer MUST be in {append.language}."
            elif hasattr(append, 'language') and append.language is None:
                formatted_prompt += " The answer MUST be in the same language as the user provided."

            if append.summarize:
                formatted_prompt += append.summarize

            if append.questioning:
                formatted_prompt += append.questioning

            if append.concise:
                formatted_prompt += append.concise

            if append.new_lines:
                formatted_prompt += append.new_lines

        return formatted_prompt

    def process_text(self, input: str):
        if input == self.options.eom:
            self.messages += self.last_user_message
            print(self.last_user_message)
            print(self.messages)
            self.last_user_message = self.add_message("user", "")
            self.send_messages()

        if input == self.options.eos:
            self.eos = True

        self.last_user_message["content"] = input

    def send_messages(self):
        stream = self.client.chat(
            model=self.active_prompt.model,
            messages=self.messages,
            stream=self.options.stream,
            keep_alive=self.active_prompt.keep_alive,
            options={
                'temperature': 0.0,
                'num_predict': 1024.0,
                'top_p': 0.1
            }
        )

        current_message = self.add_message("assistant", "")
        self.messages += current_message
        current_sentence = ""
        current_response = ""
        for chunk in stream:
            current_token = chunk['response']
            current_sentence += current_token
            current_response += current_token
            self.messages[-1]["content"] = current_response
            if self.options.stream:
                if current_token == ".":
                    print(current_sentence)
                    current_sentence = ""

        if current_response:
            print(current_response)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Executes a NLP task on a single line of text utilising ollama as a conversation")
    parser.add_argument("--config", default="./config_conversational.yaml", help="The config file to load")
    parser.add_argument("--prompt", default="default", help="The prompt's name")
    args = parser.parse_args()

    config_manager = ConversationalConfigManager(args.config)
    llm_manager = ConversationalLlmManager(config_manager.get_options(), config_manager.get_prompt(args.prompt))

    while not llm_manager.eos:
        current_line = input()
        llm_manager.process_text(current_line)
